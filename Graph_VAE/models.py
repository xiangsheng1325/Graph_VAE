import torch
from torch.nn.modules.module import Module
import torch.nn.init as init
import torch.nn as nn
import torch.nn.functional as F
from torch.autograd import Variable
from torch.nn.parameter import Parameter
import math
# models
# GCN

class GCN(Module):
    """
    GCN layer, reference to: https://arxiv.org/abs/1609.02907
    """

    def __init__(self, input_len, output_len, bias=True):
        super(GCN, self).__init__()
        self.input_len = input_len
        self.output_len = output_len
        # print("Type of ouput_len: {}\t, value of output_len: {}".format(type(output_len), output_len))
        self.W = Parameter(torch.FloatTensor(input_len, output_len))
        if bias:
            self.b = Parameter(torch.FloatTensor(output_len))
        else:
            self.register_parameter('b', None)

    def reset_parameters(self):
        """
        reset parameters in uniform distribution
        """
        margin = 1. / math.sqrt(self.W.size(1))
        self.W.data.uniform_(-margin, margin)
        if self.b is not None:
            self.b.data.uniform_(-margin.margin)

    def forward(self, inputs, adj):
        """
        When this layer is called, execute this function.
        :param inputs:  embedded node vectors (with condition context)
        :param adj:  adjacent matrix
        :return: output of GCN layer
        """
        support = torch.mm(inputs, self.W)
        # output = torch._sparse_mm(adj, support)
        output = torch.spmm(adj, support)
        if self.b is None:
            return output
        else:
            return output + self.b

    def __str__(self):
        return "Layer: {}({}->{})".format(self.__class__.__name__, self.input_len, self.output_len)

    def __repr__(self):
        return "Layer: {}({}->{})".format(self.__class__.__name__, self.input_len, self.output_len)


class GCNEncoder(nn.Module):
    def __init__(self, emb_size, hidden_dim, layer_num=2):
        super(GCNEncoder, self).__init__()
        self.act = nn.ReLU()
        self.gc_top = GCN(emb_size, hidden_dim)
        self.gc_medium = [GCN(hidden_dim, hidden_dim) for i in range(layer_num-1)]
        self.mean = nn.Sequential(nn.Linear(hidden_dim, hidden_dim//2),
                                  nn.ReLU(),
                                  nn.Linear(hidden_dim//2, hidden_dim//2))
        self.logvar = nn.Sequential(nn.Linear(hidden_dim, hidden_dim//2),
                                    nn.ReLU(),
                                    nn.Linear(hidden_dim//2, hidden_dim//2))
        for m in self.modules():
            if isinstance(m, nn.Linear):
                m.weight.data = init.xavier_uniform(m.weight.data, gain=nn.init.calculate_gain('relu'))
            elif isinstance(m, GCN):
                m.W.data = init.xavier_uniform(m.W.data, gain=nn.init.calculate_gain('relu'))

    def forward(self, x, adj, normalized=True):
        """
        call this function when you use encoder
        :param normalized: whether the adjacent matrix is normalized
        :param x: node features of one graph
        :param adj: Normalized adjacency matrix of one graph.
        :return:
        """
        if not normalized:
            adj = normalize(adj)
        x = self.gc_top(x, adj)
        x = self.act(x)
        for gcn in self.gc_medium:
            x = gcn(x, adj)
            x = self.act(x)
        mean = self.mean(x)
        logvar = self.logvar(x)
        return mean, logvar


class VanillaDecoder(nn.Module):
    def __init__(self, dropout=0.5):
        """
        InnerProductDecoder definition.
        :param dropout: probability to randomly zero some of the elements from a Bernoulli distribution.
        """
        super(VanillaDecoder, self).__init__()
        self.act = nn.Sigmoid()
        self.dropout = dropout

    def forward(self, x):
        x = F.dropout(x, p=self.dropout)
        rec = torch.mm(x, x.permute(1, 0))
        return self.act(rec)


class MLPDecoder(nn.Module):
    def __init__(self, input_dim, hidden_dim=16, dropout=0.5):
        super(MLPDecoder, self).__init__()
        self.act = nn.Sigmoid()
        self.dropout = dropout
        self.decode = nn.Sequential(nn.Linear(input_dim, hidden_dim),
                                    nn.ReLU())
        for m in self.modules():
            if isinstance(m, nn.Linear):
                m.weight.data = init.xavier_uniform(m.weight.data, gain=nn.init.calculate_gain('relu'))

    def forward(self, x):
        x = F.dropout(x, p=self.dropout)
        x = self.decode(x)
        rec = torch.mm(x, x.permute(1, 0))
        return self.act(rec)


class GraphVAE(nn.Module):
    def __init__(self, emb_size, encode_dim, layer_num, decode_dim, dropout):
        super(GraphVAE, self).__init__()
        self.encoder = GCNEncoder(emb_size=emb_size,
                                  hidden_dim=encode_dim,
                                  layer_num=layer_num)
        self.decoder = MLPDecoder(input_dim=encode_dim,
                                  hidden_dim=decode_dim,
                                  dropout=dropout)

    def remove_eye(self, adj):
        adj_ = adj
        assert ((adj_ == adj_.T).all())
        adj_ -= torch.diag(torch.diag(adjj))
        return adj_

    def forward(self, adj, x=None, normalized=True, training=True):
        if x is None:
            x = get_embedding(adj, max_size=8, method='spectral')

        mean, logvar = self.encoder(x, adj, normalized)
        noise = torch.randn(mean.shape, requires_grad=True).cuda()
        std = logvar.mul(0.5).exp_()
        if training:
            pl = []
            for j in range(mean.shape[0]):
                prior_loss = 1 + logvar[j, :] - mean[j, :].pow(2) - logvar[j, :].exp()
                # torch.numel refers to the total number of elements in the tensor, instead of the sum of elements.
                prior_loss = (-0.5 * torch.sum(prior_loss)) / torch.numel(mean[j, :].data)
                pl.append(prior_loss)
            loss_kl = sum(pl)
            x = mean + std * noise
            rec_adj = self.decoder(x)
            rec_adj = self.remove_eye(rec_adj)
            binary_cross_entropy = nn.BCELoss()
            binary_cross_entropy.cuda()
            loss_rec = binary_cross_entropy(rec_adj, Variable(torch.tensor(adj)).cuda())
            return loss_kl+loss_rec
        else:
            x = mean
            rec_adj = self.decoder(x)
            return rec_adj


